from argparse import ArgumentParser
import codecs
import copy
import csv
import logging
import os
import random
import sys

from datasets import Dataset
from nltk import wordpunct_tokenize
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm import tqdm
import torch

logic_inference_logger = logging.getLogger(__name__)
RANDOM_SEED: int = 42


def sample_to_str(en_text: str, ru_text: str) -> str:
    checked_llm_prediction = ' '.join(ru_text.strip().split())
    context = ' '.join(en_text.strip().split())
    united_prompt = 'The verified system\'s task is a machine translation.'
    united_prompt += ' The sentence generated by the verified system: '
    united_prompt += checked_llm_prediction
    if united_prompt[-1].isalnum():
        united_prompt += '.'
    united_prompt += f' The generation context: {context}'
    if united_prompt[-1].isalnum():
        united_prompt += '.'
    return united_prompt


def main():
    random.seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)

    if not torch.cuda.is_available():
        err_msg = 'CUDA is not available!'
        logic_inference_logger.error(err_msg)
        raise ValueError(err_msg)
    device = torch.device('cuda')
    torch.cuda.manual_seed(RANDOM_SEED)

    parser = ArgumentParser()
    parser.add_argument('-i', '--input', dest='input_name', type=str, required=True,
                        help='The input name of LogicInference_OA_ru dataset.')
    parser.add_argument('-o', '--output', dest='output_name', type=str, required=True,
                        help='The output name of LogicInference_OA_ru dataset with '
                             'estimated probabilities of hallucinations.')
    parser.add_argument('-m', '--model', dest='model_name', type=str, required=True,
                        help='The path to hallucination detector.')
    parser.add_argument('--batch', dest='minibatch_size', type=int, required=False, default=16,
                        help='The mini-batch size.')
    parser.add_argument('-t', '--type', dest='model_type', type=str, required=True, choices=['gpt', 'roberta'],
                        help='The model type which is used in the hallucination detector (gpt or roberta).')
    args = parser.parse_args()

    source_dataset_path = os.path.normpath(args.input_name)
    if not os.path.isdir(source_dataset_path):
        err_msg = f'The directory "{source_dataset_path}" does not exist!'
        logic_inference_logger.error(err_msg)
        raise IOError(err_msg)
    source_dataset_fname = os.path.join(source_dataset_path, 'train.csv')
    if not os.path.isfile(source_dataset_fname):
        err_msg = f'The file "{source_dataset_fname}" does not exist!'
        logic_inference_logger.error(err_msg)
        raise IOError(err_msg)

    destination_dataset_path = os.path.normpath(args.output_name)
    if not os.path.isdir(destination_dataset_path):
        err_msg = f'The directory "{destination_dataset_path}" does not exist!'
        logic_inference_logger.error(err_msg)
        raise IOError(err_msg)
    if os.path.basename(source_dataset_path) == os.path.basename(destination_dataset_path):
        err_msg = f'The destination dataset path is equal to the source one!'
        logic_inference_logger.error(err_msg)
        raise IOError(err_msg)

    destination_dataset_fname = os.path.join(destination_dataset_path, 'train.csv')

    model_name = os.path.normpath(args.model_name)
    if not os.path.isdir(model_name):
        err_msg = f'The directory "{model_name}" does not exist!'
        logic_inference_logger.error(err_msg)
        raise IOError(err_msg)

    true_header = ['INSTRUCTION_EN', 'RESPONSE_EN', 'INSTRUCTION_RU', 'RESPONSE_RU', 'TRANSLATION_SCORE']
    loaded_header = []
    with codecs.open(source_dataset_fname, mode='r', encoding='utf-8', errors='ignore') as fp:
        data_reader = csv.reader(fp, delimiter=',', quotechar='"')
        source_samples = list(filter(lambda it: len(it) > 0, data_reader))
        if len(source_samples) == 0:
            err_msg = f'The file "{source_dataset_fname}" is empty!'
            logic_inference_logger.error(err_msg)
            raise IOError(err_msg)
        if len(source_samples[0]) < len(true_header):
            err_msg = (f'The file "{source_dataset_fname}" has a wrong header! '
                       f'Expected {true_header}, got {source_samples[0]}.')
            logic_inference_logger.error(err_msg)
            raise IOError(err_msg)
        if source_samples[0][:len(true_header)] != true_header:
            err_msg = (f'The file "{source_dataset_fname}" has a wrong header! '
                       f'Expected {true_header}, got {source_samples[0]}.')
            logic_inference_logger.error(err_msg)
            raise IOError(err_msg)
        loaded_header = copy.copy(source_samples[0])
        if len(source_samples) < 2:
            err_msg = f'The file "{source_dataset_fname}" is empty!'
            logic_inference_logger.error(err_msg)
            raise IOError(err_msg)
        source_samples_without_header = source_samples[1:]
    del source_samples

    if args.model_type == 'roberta':
        hallucination_detector = pipeline(
            task='text-classification',
            model=model_name,
            framework='pt', trust_remote_code=True, device='cuda', torch_dtype=torch.float32
        )

        input_texts = []
        for val in source_samples_without_header:
            instruction_en = val[0]
            response_en = val[1]
            instruction_ru = val[2]
            response_ru = val[3]
            input_texts += [
                sample_to_str(en_text=instruction_en, ru_text=instruction_ru),
                sample_to_str(en_text=response_en, ru_text=response_ru)
            ]
        input_dataset = Dataset.from_dict({'text': input_texts})
        del input_texts

        probabilities = []
        for out in tqdm(hallucination_detector(KeyDataset(input_dataset, 'text'), batch_size=args.minibatch_size,
                                               padding='longest'), total=len(input_dataset)):
            if out['label'] == 'Hallucination':
                hallucination_probability = out['score']
            else:
                hallucination_probability = 1.0 - out['score']
            probabilities.append(hallucination_probability)

        with codecs.open(destination_dataset_fname, mode='w', encoding='utf-8', buffering=0) as fp:
            data_writer = csv.writer(fp, delimiter=',', quotechar='"')
            data_writer.writerow(loaded_header + ['p(Hallucination)'])
            for idx, val in enumerate(source_samples_without_header):
                proba = max(probabilities[idx * 2], probabilities[idx * 2 + 1])
                data_writer.writerow(list(val) + [str(round(proba, 6))])
    else:
        device = 'cuda:0'
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        system_prompt = 'You are an experienced professional translator. ' \
                        'Can you tell me if this translation corresponds to the original text? ' \
                        'Answer using ONLY yes or no, please. I\'m going to tip $200 for your perfect answer!'
        user_prompt_template = 'The original text: {en_text}\nThe translation: {ru_text}'
        with codecs.open(destination_dataset_fname, mode='w', encoding='utf-8', buffering=0) as fp:
            data_writer = csv.writer(fp, delimiter=',', quotechar='"')
            data_writer.writerow(loaded_header + ['HALLUCINATION'])
            for val in tqdm(source_samples_without_header):
                instruction_en = val[0]
                response_en = val[1]
                instruction_ru = val[2]
                response_ru = val[3]
                messages = [
                    {
                        'role': 'system',
                        'content': system_prompt
                    },
                    {
                        'role': 'user',
                        'content': user_prompt_template.format(en_text=instruction_en, ru_text=instruction_ru)
                    }
                ]
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = tokenizer([text], return_tensors='pt').to(device)
                generated_ids = model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=512
                )
                generated_ids = [
                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
                ]
                response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
                del text, model_inputs, generated_ids, messages
                words_in_response = set(wordpunct_tokenize(response.lower()))
                if ('no' in words_in_response) or ('yes' not in words_in_response):
                    hallucination = True
                else:
                    messages = [
                        {
                            'role': 'system',
                            'content': system_prompt
                        },
                        {
                            'role': 'user',
                            'content': user_prompt_template.format(en_text=response_en, ru_text=response_ru)
                        }
                    ]
                    text = tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = tokenizer([text], return_tensors='pt').to(device)
                    generated_ids = model.generate(
                        model_inputs.input_ids,
                        max_new_tokens=512
                    )
                    generated_ids = [
                        output_ids[len(input_ids):] for input_ids, output_ids in
                        zip(model_inputs.input_ids, generated_ids)
                    ]
                    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    del text, model_inputs, generated_ids, messages
                    words_in_response = set(wordpunct_tokenize(response.lower()))
                    hallucination = (('no' in words_in_response) or ('yes' not in words_in_response))
                data_writer.writerow(list(val) + ['yes' if hallucination else 'no'])


if __name__ == '__main__':
    logic_inference_logger.setLevel(logging.INFO)
    fmt_str = '%(filename)s[LINE:%(lineno)d]# %(levelname)-8s ' \
              '[%(asctime)s]  %(message)s'
    formatter = logging.Formatter(fmt_str)
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setFormatter(formatter)
    logic_inference_logger.addHandler(stdout_handler)
    file_handler = logging.FileHandler('logic_inference_hallucination.log')
    file_handler.setFormatter(formatter)
    logic_inference_logger.addHandler(file_handler)
    main()
